# モンテカルロ法(2) 発展的な話題

* エラーバーについて
* 不偏推定量とJackknife法
* クラスターアルゴリズムとImproved Estimator

## エラーバー

### エラーバーとは

実験において、まったく同じ条件下でも、観測するたびに結果が揺らぐ。そこで何度も観測し、「その観測値がどの程度確からしいか」を推定する。たとえば、ある回路の電流を6回測定し、$1.16, 1.13, 1.12, 1.12, 1.11, 1.08$という値が得られたとしよう(単位はmA)。この回路の電流の測定値は、平均値と「平均値の標準偏差」を使って

$$
1.12 \pm 0.01 \quad mA
$$

と書くように習ったかと思う。これを見ると、大雑把に「1.1 までは自信があるが、1.12 の値はあまり自信がなく、1.11かもしれないし、1.13かもしれない」ということがわかる。そこで、グラフでは$1.12$のところにシンボルを置き、$1.11$から$1.13$まで棒を描画して、「観測範囲はこの範囲にありそうだよ」ということを表現する。そのことから$\pm$記号の後の数値を「エラーバー」と呼ぶ。

このエラーバーを、単に「観測データの自信度である」と解釈し、「小さければ小さいほどよい」と思ってしまう人がいる。しかし、エラーバーの役割は「観測データの自信度」にとどまらず、様々な役割を持っている。以下では、エラーバーとは何か、どのような性質を持っているかを解説する。

まず、エラーバーを定義しよう。観測するたびに値が変化する観測量を確率変数$\hat{X}$とする。これを$N$回観測し、$i$番目の観測データを$X_i$としよう。確率変数$\hat{X}$の平均値$\bar{X}$と分散$\sigma^2$は以下のように与えられる。

$$
\bar{X} = \frac{1}{N} \sum_i X_i \equiv
$$

$$
\sigma^2 = \frac{1}{N-1} \sum_i (X_i - \bar{X})^2
$$

この式の意味を考えよう。確率変数$\hat{X}$について、無限回観測したとして、その観測値の集合を　**母集団(population)** と呼ぶ。観測とは、母集団から有限個の要素を取り出すことであり、個々の要素を標本と呼ぶ。$N$回観測して得られた$N$個の集合$X_i$が標本である。我々の目的は、この標本から、様々な統計的な量を推定することだ。例えば先程求めた平均値$E[\hat{X}]$や分散$V[\hat{X}]$は、母集団を特徴づける特徴量である。このように、特徴量を標本$\{X_i\}$から推定することを **統計的推定(statistical estimation)** 、もしくは単に推定と呼ぶ。そして、標本から特徴量を推定する関数ことを **推定量(estimator)** と呼ぶ。数値計算ではestimatorと呼ぶことが多いので、以下では「推定量」と呼ばずにestimatorで統一する。先ほどの式では、左辺が特徴量、右辺がestimatorである。

さて、確率変数の平均値$\bar{X}$もまた確率変数となるが、その分布は、標本数が増えればガウス分布へと近づいていく(中心極限定理)。我々が推定したいのは、確率変数$\hat{X}$が従う母集団の分布ではなく、「平均値が従う分布」であり、その特徴量である。

まず、我々が知りたい母集団とは何かを定義しておこう。確率変数$\hat{X}$の取る値が、ある実数$a$と$b$の間にある確率が以下のような積分で表されるとする。

$$
P(a < \hat{X} < b) = \int_a^b f(x)dx
$$

この時、関数$f(x)$を **確率密度関数(probability distribution function, PDF)** と呼ぶ。我々が知りたい「母集団の分布」とは、この確率密度関数のことだ。特に知りたいのは、この関数の1次と2次のモーメント、$\mu$と$\sigma^2$である。

$$
\mu = \int x f(x) dx
$$

$$
\sigma^2 = \int (x-\mu)^2 f(x) dx
$$

この量を$N$個の標本$X_i$から求めたい。

まず、$N$個の標本$X_i$から、「平均値が従う分布」の平均値と分散のesitomatorは以下のように与えられる。

$$
\bar{X} = \frac{1}{N}\sum_i X_i
$$

$$
V[\bar{X}] = \frac{1}{N(N-1)} \sum_i (X_i-\bar{X})^2
$$

期待値のestimatorは母集団のものと同じだが、分散は$1/N$となる。また、$\hat{X}$の母集団は正規分布とは限らないが、$N$が大きい時、$\bar{X}$の従う分布は正規分布に近づいていく。それを見てみよう。

例によって「公平なサイコロ」を考える。サイコロを振った後出てくる目を確率変数$\hat{X}$とみなそう。$\hat{X}$がとり得る値は$1,2,3,4,5,6$の6種類で、それぞれ出現確率は等しく$1/6$だ。したがって、その母集団は以下のような分布になる。

![population](histogram/population.png)

サイコロを二つ振った時の平均はどうなるだろう？サイコロの出る目の集団は$(1,1)$から$(6,6)$まで36通りだが、目の平均は$1$から$6$まで11通りだ。分布を見てみよう。

```py
from matplotlib import pyplot as plt
import numpy as np
import itertools
x = np.arange(1,7)
p= itertools.product(x, x)
plt.hist([sum(t)/len(t) for t in p])
```

![n2](histogram/n2.png)

サイコロに4回振った場合、出る目の集団は$(1,1,1,1)$から$(6,6,6,6)$まで$6^4 = 1296$通りだが、その平均は$1$から$6$まで21通りとなる。分布はこうなる。

```py
from matplotlib import pyplot as plt
import numpy as np
import itertools
x = np.arange(1,7)
p= itertools.product(x, x, x, x)
plt.hist([sum(t)/len(t) for t in p])
```

![n4](histogram/n4.png)

これ以上は厳密にやると大変なので、サンプリングで分布を推定しよう。

![n4](histogram/n4.png)

```py
n = 20000
a = []
for _ in range(n):
    xi = np.average(np.random.randint(1,7,1000))
    a.append(xi)
plt.hist(a,bins=40)
```

![n1000](histogram/n1000.png)

平均を取る数$N$が$10,100,1000$のそれぞれの場合を重ねてプロットするとこうなる。

![compare](histogram/compare.png)

それぞれサイコロを$N$回振って平均を取る処理を20000回繰り返してヒストグラムをとったものだ。
$N$が大きくなるほど分布が狭くなっていくのがわかる。すなわち、標本の数が増えるほど、「平均値の母集団」の分布の分散は小さくなり、正規分布へと近づいていく。

平均$\mu$、分散$\sigma^2$の正規分布は以下の式で表される。

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2} \right)
$$

さて、確率密度関数の定義は、確率変数$\hat{X}$の値がある範囲$a < \hat{X} < b$にある確率が、その範囲における確率密度関数の積分で与えられる、というものであった。したがって、平均$\mu$、分散$\sigma^2$の正規分布に従う確率変数$\hat{X}$が、平均の前後$\sigma$の範囲に収まる確率は

$$
P(\mu - \sigma < \hat{X} < \mu + \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} \int_{\mu - \sigma}^{\mu + \sigma} \exp\left(\frac{(x-\mu)^2}{2\sigma^2} \right) dx \sim 0.6827
$$

で与えられる。これを **1シグマの範囲** と呼ぶ。確率変数が正規分布に従う時、1シグマの範囲に入る確率は68.27%である。平均を50、標準偏差を10とするようにスケールした時の値が偏差値であるから、これは「偏差値40から60の間にいるのは全体の68.27%である」ということを意味する。逆に、偏差値40以下、もしくは60以上の人は31.73%であるから、偏差値60以上の人は15.87%ほどいることになる。

同様に、平均の前後$n \sigma$に収まる範囲を「nシグマの範囲」と呼ぶ。2シグマの範囲に入る確率、すなわち、偏差値30から70の間に入る確率が95.45%である。簡単にまとめて置こう。

* 1シグマ：入る確率 68.27% 　 外れる確率 31.73%
* 2シグマ：入る確率 95.45% 　 外れる確率 4.55%
* 3シグマ：入る確率 99.73% 　 外れる確率 0.27%
* 5シグマ：入る確率 99.9994%  外れる確率 0.0005%

ここまでまとめておこう。

* エラーバーとは観測値を確率変数とみなした時に、その平均値の分布の推定標準偏差のこと
* サンプル数を増やせば増やすほど、エラーバーは小さくなる
* 観測値が独立同分布なら、サンプル数を増やしていくと平均値の分布はガウス分布に漸近する
* 平均$\mu$、分散$\sigma^2$のガウス分布に従う確率変数$\hat{X}$について、$\mu - n \sigma < \hat{X} < \mu + n \sigma$をnシグマの範囲と呼ぶ。
* ガウス分布に従う確率変数が独立であるならば
    * 「1シグマの範囲」からは3つに1つは外れる
    * 「5シグマの範囲」から外れる確率はほぼゼロ


### エラーバーがおかしなグラフ

エラーバーつきの期待値は、もしデータが無相関でガウス分布に従うのであれば、以下の性質を持つ。

* 「真の値」の上下に均等にばらつく
* 3つに1つは「真の値」がエラーバーの範囲に入らない
* 「真の値」がエラーバーの2倍離れることは稀、5倍離れることはない。

観測量について「真の値」を定義するのは難しく、その言葉の使用は本来気を付けるべきではあるが、ここでは便宜上、「観測量を確率変数とみなした時の母集団の分布の平均」のことを「真の値」と呼ぶことにする。

エラーバーの意味は「おおむねこのあたりに値があると思うが、エラーバーの範囲くらいで自信がないよ」ということなので、「小さければ小さいほど良い」と思う人がいる。そういう側面もあるが、エラーバーにはもっと重要な役割がある。それはデータの異常検知だ。エラーバーが適切についていればデータを見て「ん？おかしいぞ」と気づくことができる。以下、エラーバーがおかしいグラフについて実例を見ながら、「何がおかしいか」を確認しておこう。

#### エラーバーが大きすぎるグラフ

![exp1](case1/exp1.png)

何かが指数関数的に減衰しているように見える。このグラフに「ありそうな線」を重ねてみよう。

![exp1_line](case1/exp1_line.png)

この「ありそうな線」は、「データの精度を高くしていったときに収束するであろう線」、すなわち「真の値」だ。もちろん「真の値」はわからないが、データから「だいたいこのあたりだろう」ということは推定できる。

さて、もしこのエラーバーが1シグマの範囲で取られており、かつデータが独立なガウス分布に従うのなら、3つに1つは「真の値」から外れなければならない。にもかかわらず、全てのエラーバーが「ありそうな線」にかかってしまっており、慣れていればここで「変だな」と気づく。

このデータは以下のコードで生成されたものだ。

```py
import numpy as np

N = 10
np.random.seed(1)
for i in range(10):
    x = i + 0.5
    d = np.zeros(N)
    d += np.exp(-x/3)
    d += np.random.randn(N)*0.1
    y = np.average(d)
    e = np.std(d)
    print(f"{x} {y} {e}")
```

ここで、`numpy.std`で標準偏差を求めており、それをそのままエラーバーのデータとして使ってしまっている。初心者がよくやるミスだ。正しくは$\sqrt{N}$で割ってやらないといけない。

```py
import numpy as np

N = 10
np.random.seed(1)
for i in range(10):
    x = i + 0.5
    d = np.zeros(N)
    d += np.exp(-x/3)
    d += np.random.randn(N)*0.1
    y = np.average(d)
    e = np.std(d)/np.sqrt(N)
    print(f"{x} {y} {e}")
```

こうして生成されたデータからグラフを作成すると以下のようになる。

![exp2_line](case1/exp2_line.png)

このグラフでは10個のうち少なくとも一つはデータが外れており、他にもエラーバーギリギリのデータ点もあるため、さほど不自然には見えない。

#### 偏っているグラフ

ある観測値について、サンプル数を増やすことで期待値が「真の値」に収束し、かつエラーバーが小さくなることを確認する目的で、以下のようなグラフを作ったとする。

![corr1](case2/corr1.png)

どうやらサンプル数が多い極限で0.5に収束しそうなデータであることがわかる。なので0.5の線をひいてみよう。

![corr1](case2/corr1_line.png)

もし各データ点が独立であるなら、「真の値」の両側に均等にばらつくはずだ。しかし、このグラフでは最初の3点が「同じ側」に外れている。

このデータは以下のコードで生成されたものだ。

```py
import numpy as np

np.random.seed(1)
N = 2048
d = np.random.random(N)
for i in range(4, 12):
    n = 2**i
    dd = d[:n]
    ave = np.average(dd)
    err = np.std(dd)/np.sqrt(n)
    print(f"{n} {ave} {err}")
```

これでは異なるデータ点が同じデータを使用しているため、データ点が独立ではなくなる。

データ点ごとに独立なデータを利用するように修正したコードは以下の通り。

```py
import numpy as np

np.random.seed(1)
N = 2048
for i in range(4, 12):
    n = 2**i
    dd = np.random.random(n)
    ave = np.average(dd)
    err = np.std(dd)/np.sqrt(n)
    print(f"{n} {ave} {err}")
```

このコードが生成したデータのグラフは以下の通り。

![corr2](case2/corr2_line.png)

こちらではデータが「真の値」の両側にばらついており、もっともらしい。

#### エラーバーが小さすぎるグラフ

![langevin](case3/langevin.png)

![langevin_line](case3/langevin_line.png)

```py
import numpy as np

N = 1000
v = 0.0
gamma = 0.1
np.random.seed(1)

for j in range(10):
    d = np.zeros(N)
    for i in range(N):
        v += np.random.randn()*0.1
        v -= gamma * v
        d[i] = v
    ave = np.average(d)
    err = np.std(d) / np.sqrt(N)
    print(f"{(j+0.5)*N} {ave} {err}")
```