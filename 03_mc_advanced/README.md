# モンテカルロ法(2) 発展的な話題

* エラーバーについて
* 不偏推定量とJackknife法
* クラスターアルゴリズムとImproved Estimator

## エラーバー

### エラーバーとは

実験において、まったく同じ条件下でも、観測するたびに結果が揺らぐ。そこで何度も観測し、「その観測値がどの程度確からしいか」を推定する。たとえば、ある回路の電流を6回測定し、$1.16, 1.13, 1.12, 1.12, 1.11, 1.08$という値が得られたとしよう(単位はmA)。この回路の電流の測定値は、平均値と「平均値の標準偏差」を使って

$$
1.12 \pm 0.01 \quad mA
$$

と書くように習ったかと思う。これを見ると、大雑把に「1.1 までは自信があるが、1.12 の値はあまり自信がなく、1.11かもしれないし、1.13かもしれない」ということがわかる。そこで、グラフでは$1.12$のところにシンボルを置き、$1.11$から$1.13$まで棒を描画して、「観測範囲はこの範囲にありそうだよ」ということを表現する。そのことから$\pm$記号の後の数値を「エラーバー」と呼ぶ。

このエラーバーを、単に「観測データの自信度である」と解釈し、「小さければ小さいほどよい」と思ってしまう人がいる。しかし、エラーバーの役割は「観測データの自信度」にとどまらず、様々な役割を持っている。以下では、エラーバーとは何か、どのような性質を持っているかを解説する。

まず、エラーバーを定義しよう。観測するたびに値が変化する観測量を確率変数$\hat{X}$とする。これを$N$回観測し、$i$番目の観測データを$X_i$としよう。確率変数$\hat{X}$の平均値$\bar{X}$と分散$\sigma^2$は以下のように与えられる。

$$
\bar{X} = \frac{1}{N} \sum_i X_i \equiv
$$

$$
\sigma^2 = \frac{1}{N-1} \sum_i (X_i - \bar{X})^2
$$

この式の意味を考えよう。確率変数$\hat{X}$について、無限回観測したとして、その観測値の集合を　**母集団(population)** と呼ぶ。観測とは、母集団から有限個の要素を取り出すことであり、個々の要素を標本と呼ぶ。$N$回観測して得られた$N$個の集合$X_i$が標本である。我々の目的は、この標本から、様々な統計的な量を推定することだ。例えば先程求めた平均値$E[\hat{X}]$や分散$V[\hat{X}]$は、母集団を特徴づける特徴量である。このように、特徴量を標本$\{X_i\}$から推定することを **統計的推定(statistical estimation)** 、もしくは単に推定と呼ぶ。そして、標本から特徴量を推定する関数ことを **推定量(estimator)** と呼ぶ。数値計算ではestimatorと呼ぶことが多いので、以下では「推定量」と呼ばずにestimatorで統一する。先ほどの式では、左辺が特徴量、右辺がestimatorである。

さて、確率変数の平均値$\bar{X}$もまた確率変数となるが、その分布は、標本数が増えればガウス分布へと近づいていく(中心極限定理)。我々が推定したいのは、確率変数$\hat{X}$が従う母集団の分布ではなく、「平均値が従う分布」であり、その特徴量である。

まず、我々が知りたい母集団とは何かを定義しておこう。確率変数$\hat{X}$の取る値が、ある実数$a$と$b$の間にある確率が以下のような積分で表されるとする。

$$
P(a < \hat{X} < b) = \int_a^b f(x)dx
$$

この時、関数$f(x)$を **確率密度関数(probability distribution function, PDF)** と呼ぶ。我々が知りたい「母集団の分布」とは、この確率密度関数のことだ。特に知りたいのは、この関数の1次と2次のモーメント、$\mu$と$\sigma^2$である。

$$
\mu = \int x f(x) dx
$$

$$
\sigma^2 = \int (x-\mu)^2 f(x) dx
$$

この量を$N$個の標本$X_i$から求めたい。

まず、$N$個の標本$X_i$から、「平均値が従う分布」の平均値と分散のesitomatorは以下のように与えられる。

$$
\bar{X} = \frac{1}{N}\sum_i X_i
$$

$$
V[\bar{X}] = \frac{1}{N(N-1)} \sum_i (X_i-\bar{X})^2
$$

期待値のestimatorは母集団のものと同じだが、分散は$1/N$となる。また、$\hat{X}$の母集団は正規分布とは限らないが、$N$が大きい時、$\bar{X}$の従う分布は正規分布に近づいていく。それを見てみよう。

例によって「公平なサイコロ」を考える。サイコロを振った後出てくる目を確率変数$\hat{X}$とみなそう。$\hat{X}$がとり得る値は$1,2,3,4,5,6$の6種類で、それぞれ出現確率は等しく$1/6$だ。したがって、その母集団は以下のような分布になる。

![population](histogram/population.png)

サイコロを二つ振った時の平均はどうなるだろう？サイコロの出る目の集団は$(1,1)$から$(6,6)$まで36通りだが、目の平均は$1$から$6$まで11通りだ。分布を見てみよう。

```py
from matplotlib import pyplot as plt
import numpy as np
import itertools
x = np.arange(1,7)
p= itertools.product(x, x)
plt.hist([sum(t)/len(t) for t in p])
```

![n2](histogram/n2.png)

サイコロに4回振った場合、出る目の集団は$(1,1,1,1)$から$(6,6,6,6)$まで$6^4 = 1296$通りだが、その平均は$1$から$6$まで21通りとなる。分布はこうなる。

```py
from matplotlib import pyplot as plt
import numpy as np
import itertools
x = np.arange(1,7)
p= itertools.product(x, x, x, x)
plt.hist([sum(t)/len(t) for t in p])
```

![n4](histogram/n4.png)

これ以上は厳密にやると大変なので、サンプリングで分布を推定しよう。

![n4](histogram/n4.png)

```py
n = 20000
a = []
for _ in range(n):
    xi = np.average(np.random.randint(1,7,1000))
    a.append(xi)
plt.hist(a,bins=40)
```

![n1000](histogram/n1000.png)

平均を取る数$N$が$10,100,1000$のそれぞれの場合を重ねてプロットするとこうなる。

![compare](histogram/compare.png)

それぞれサイコロを$N$回振って平均を取る処理を20000回繰り返してヒストグラムをとったものだ。
$N$が大きくなるほど分布が狭くなっていくのがわかる。すなわち、標本の数が増えるほど、「平均値の母集団」の分布の分散は小さくなり、正規分布へと近づいていく。

平均$\mu$、分散$\sigma^2$の正規分布は以下の式で表される。

$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(\frac{x^2}{2\sigma^2} \right)
$$

### エラーバーがおかしなグラフ

エラーバーつきの期待値は、もしデータが無相関でガウス分布に従うのであれば、以下の性質を持つ。

* 「真の値」の上下に均等にばらつく
* 3つに1つは「真の値」がエラーバーの範囲に入らない
* 「真の値」がエラーバーの2倍離れることは稀、3倍離れることはない。
